# 多LLM对话系统设计文档 (2025年6月更新版)

## 1. 项目概述

基于多个大型语言模型平台的对话系统，让不同AI模型就同一话题进行讨论，通过多方对话探索深入理解话题的效果。

## 2. 支持的平台和最新模型列表

### 2.1 阿里云百炼 (Alibaba Cloud Model Studio)
- **平台地址**: https://help.aliyun.com/zh/model-studio/
- **模型列表**: https://help.aliyun.com/zh/model-studio/models
- **控制台**: https://bailian.console.aliyun.com/

#### 推荐模型 (2025年最新):
1. **qwen-max-2024-09-19** (推荐旗舰模型)
   - 最强性能，32K上下文窗口
   - 适合复杂推理和创作任务
   
2. **qwen-plus-2024-09-19** (均衡选择)
   - 性能与成本均衡，32K上下文窗口
   - 适合日常对话和分析任务
   
3. **qwen-turbo-2024-11-01** (高速模型)
   - 响应速度快，128K上下文窗口
   - 适合快速交互场景
   
4. **qwen-long-2024-09-19** (长文本模型)
   - 1M上下文窗口，适合长文档处理
   
5. **qwen2.5-72b-instruct** (开源版本)
   - 开源模型，可本地部署

### 2.2 火山豆包 (Volcano Engine Doubao)
- **平台地址**: https://www.volcengine.com/product/doubao  
- **控制台**: https://console.volcengine.com/ark
- **文档**: https://www.volcengine.com/docs/82379

#### 开通的模型

已接入如下模型，模型 ID 和接入点如下：
- 模型 ID: doubao-seed-1-6-250615
- 接入点: ep-m-20250629223026-prr94

#### 推荐模型 (2025年最新):
1. **doubao-seed-1.6** (🌟 最新旗舰模型，推荐)
   - 多模态深度思考模型，256K上下文
   - 支持文本、图像、视频理解
   - 同时支持thinking、non-thinking、auto三种思考模式
   
2. **doubao-seed-1.6-flash** (高速版本)
   - 极致推理速度，256K上下文
   - 保持高性能的同时提升响应速度
   
3. **doubao-seed-1.6-thinking** (深度思考增强版)
   - 强化思考能力，256K上下文
   - 在编程、数学、逻辑推理等基础能力上进一步提升
   
4. **doubao-1.5-pro-32k** (经典推荐)
   - 32K上下文，综合性能优秀
   - 性价比高，适合生产环境
   
5. **doubao-1.5-lite-32k** (轻量高效)
   - 32K上下文，轻量级模型
   - 成本较低，适合高频调用
   
6. **doubao-1.5-pro-256k** (长文本版本)
   - 256K超长上下文窗口
   
7. **doubao-1.5-vision-pro** (视觉理解模型)
   - 支持图像理解和分析

#### 🔥 重要配置说明

**火山豆包有两种接入方式**：

1. **预置推理接入点**（推荐，即开即用）：
   - 直接使用预置模型ID，如：`doubao-seed-1.6`
   - 无需额外配置，开通即可使用
   - 预置模型ID列表：https://www.volcengine.com/docs/82379/1330310

2. **自定义推理接入点**（高级功能）：
   - 需要在控制台创建推理接入点
   - 获得Endpoint ID（格式：`ep-20241230102630-xxxxx`）
   - 支持精调模型、权限控制、算力保障等功能
   - 创建指南：https://www.volcengine.com/docs/82379/1099522

**配置建议**：
- 初次使用推荐选择预置模型ID：`doubao-seed-1.6`
- 如需特殊功能，可创建自定义推理接入点

#### 常见问题排查

**404错误**：
- 检查模型名称是否正确（区分大小写）
- 确认使用的是预置模型ID而非描述性名称
- 如使用Endpoint ID，确保格式正确且已创建

**推荐配置**：
```bash
DOUBAO_API_KEY=your_api_key_here
DOUBAO_MODEL=doubao-seed-1.6
```

### 2.3 月之暗面 (Moonshot AI)
- **平台地址**: https://kimi.ai/
- **控制台**: https://platform.moonshot.cn/
- **API文档**: https://platform.moonshot.cn/docs

#### 推荐模型 (2025年最新):
1. **moonshot-v1-128k** (推荐使用)
   - 128K超长上下文窗口
   - 支持超长文本处理，最高2M字符输入
   
2. **moonshot-v1-32k** (标准版本)
   - 32K上下文窗口
   - 均衡的性能和成本
   
3. **moonshot-v1-8k** (基础版本)
   - 8K上下文窗口
   - 基础对话和简单任务

**特色功能**: 
- 支持超长文本处理能力
- 专门优化的中文理解
- 强大的文档分析能力

### 2.4 DeepSeek (深度求索)
- **平台地址**: https://www.deepseek.com/
- **控制台**: https://platform.deepseek.com/
- **API文档**: https://api-docs.deepseek.com/

#### 推荐模型 (2025年最新):
1. **deepseek-reasoner** (推荐使用，R1-0528)
   - 最新推理模型，强大的逻辑推理能力
   - 优秀的数学和科学问题解答能力
   - 支持复杂的多步推理
   
2. **deepseek-chat** (对话模型，V3-0324)
   - 685B参数的混合专家模型
   - 均衡的对话和生成能力
   - 64K上下文窗口

**特色功能**:
- R1推理模型具备强大的逻辑推理和数学能力
- 提供峰谷定价，非高峰期有大幅折扣
- 开源友好，部分模型完全开源

### 2.5 Ollama (本地模型)
- **平台地址**: https://ollama.com/
- **项目地址**: https://github.com/ollama/ollama
- **文档**: https://github.com/ollama/ollama/blob/main/README.md

#### 推荐模型 (2025年最新):
1. **llama3.1:latest** (推荐通用模型)
   - Meta Llama 3.1，优秀的通用对话能力
   - 支持多种参数规模：8B、70B等
   
2. **qwen2.5:latest** (阿里开源模型)
   - 阿里通义千问开源版本
   - 中文理解能力强，性能优秀
   
3. **deepseek-coder:latest** (编程专用)
   - DeepSeek专为编程任务优化
   - 代码生成和理解能力突出
   
4. **mistral:latest** (欧洲模型)
   - Mistral AI开源模型
   - 多语言支持，性能均衡

**特色功能**:
- 完全本地化，无需API密钥
- 支持多种开源模型
- 数据隐私保护，不依赖外部服务
- 支持GPU加速，推理速度快
- 模型可离线使用

**安装配置**:
1. 安装 Ollama: https://ollama.com/download
2. 启动服务: `ollama serve`
3. 下载模型: `ollama pull llama3.1:latest`
4. 设置环境变量: `OLLAMA_ENABLED=true`
5. 重启应用即可使用

## 3. 系统架构

### 3.1 核心组件
- **配置管理** (`config.py`): 统一管理各平台API配置
- **客户端实现** (`client.py`): 各平台统一接口实现
- **对话管理** (`conversation.py`): 多方对话协调和状态管理
- **用户界面** (`app.py`): Gradio Web界面

### 3.2 技术特性
- 异步并发处理
- 统一的OpenAI SDK接口
- 完善的错误处理和重试机制
- **实时流式响应** - 支持流式显示模型回复过程
- 实时进度监控
- 对话历史保存和导出
- **响应式UI设计** - 自适应浏览器窗口大小
- 本地模型支持 - 支持Ollama本地模型

### 3.3 流式响应机制
系统支持流式响应功能，提供更好的用户体验：

#### 流式响应特性
- **实时显示**: 模型生成内容时实时显示，无需等待完整回复
- **进度可视**: 清晰显示当前回复进度和状态
- **容错处理**: 流式失败时自动降级到普通模式
- **UI优化**: 专门的流式内容显示区域

#### 实现机制
1. **客户端流式支持**: 所有LLM客户端都支持`stream_chat`方法
2. **异步处理**: 使用异步生成器处理流式数据
3. **进度回调**: 通过回调机制向UI传递流式更新
4. **状态管理**: 维护流式状态，支持中断和恢复

### 3.4 响应式UI设计
全新的响应式用户界面设计：

#### UI特性
- **全屏适配**: 自动扩展到浏览器窗口大小
- **响应式布局**: 智能调整列宽比例(1:3)
- **增强样式**: 更好的视觉效果和用户体验
- **实时更新**: 0.5秒间隔的UI更新，保证流畅性

#### 显示增强
- **流式内容区**: 专门的流式回复显示区域
- **进度指示**: 清晰的轮次和状态显示
- **平台标识**: 不同平台的emoji标识
- **滚动优化**: 智能滚动和内容管理

## 4. 更新日志 (2025年1月)

### 4.1 新增功能
本次更新基于 `docs/changelog/2025.07.05.md` 的需求，实现了以下核心功能：

#### 🆕 本地模型支持
- **Ollama集成**: 完整支持Ollama本地模型服务
- **配置简化**: 通过环境变量轻松配置本地模型
- **模型推荐**: 提供多种经过验证的开源模型选择
- **离线运行**: 支持完全离线的对话体验

#### 🚀 流式响应系统
- **实时显示**: 模型回复过程实时显示，提升用户体验
- **流式优化**: 0.01秒间隔的流式更新，保证响应流畅
- **容错机制**: 流式失败时自动降级到普通模式
- **状态管理**: 完善的流式状态跟踪和管理

#### 🎨 界面增强
- **全屏适配**: UI自动扩展至浏览器窗口大小
- **响应式设计**: 智能布局调整，适配不同屏幕尺寸
- **视觉优化**: 增强的样式和用户体验
- **更新频率**: 0.5秒间隔的UI更新，保证实时性

### 4.2 技术改进
- **异步优化**: 更好的异步处理机制
- **错误处理**: 增强的错误处理和用户友好提示
- **性能提升**: 优化的资源使用和响应速度
- **代码结构**: 更清晰的模块划分和代码组织
- **参与者限制**: 将最大参与者数量从4增加到8，支持所有平台同时参与

### 4.3 配置文件更新
- 新增 `OLLAMA_*` 环境变量配置
- 更新 `env.example` 文件，包含完整的Ollama配置说明
- 支持本地模型的灵活配置

### 4.4 Bug修复
- **参与者限制问题**: 修复了因添加Ollama平台后，系统无法支持5个参与者的问题
- **UI显示优化**: 为Ollama平台添加了专用的🏠emoji标识
- **错误处理**: 改进了参与者数量验证的错误提示

## 5. 使用指南

### 5.1 环境配置
1. 复制 `env.example` 到 `.env`
2. 配置所需的LLM平台API密钥
3. 可选：配置Ollama本地模型支持

### 5.2 Ollama本地模型设置
```bash
# 1. 安装Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. 启动服务
ollama serve

# 3. 下载模型
ollama pull llama3.1:latest

# 4. 配置环境变量
OLLAMA_ENABLED=true
OLLAMA_MODEL=llama3.1:latest
```

### 5.3 启动应用
```bash
# 安装依赖
uv install

# 启动应用
uv run python -m llm_chats.app
```

## 6. 技术架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                          Web UI (Gradio)                       │
│                     响应式全屏界面 (最多8个参与者)              │
└─────────────────────────┬───────────────────────────────────────┘
                          │
┌─────────────────────────▼───────────────────────────────────────┐
│                    对话管理器                                   │
│                  (ConversationManager)                         │
│                    支持流式响应                                 │
└─────────────────────────┬───────────────────────────────────────┘
                          │
┌─────────────────────────▼───────────────────────────────────────┐
│                    LLM客户端工厂                                │
│                  (LLMClientFactory)                            │
└─────┬──────┬──────┬──────┬──────┬─────────────────────────────────┘
      │      │      │      │      │
┌─────▼──┐ ┌─▼──┐ ┌─▼──┐ ┌─▼──┐ ┌─▼──────┐
│阿里云🔵│ │豆包🔴│ │月之🌙│ │深度🤖│ │Ollama🏠│
│百炼    │ │    │ │暗面  │ │求索  │ │(本地) │
└────────┘ └────┘ └──────┘ └──────┘ └───────┘
     │       │      │      │        │
┌────▼───────▼──────▼──────▼────────▼─────────────────────────────┐
│                    流式响应处理                                │
│              实时更新 + 容错降级                               │
└─────────────────────────────────────────────────────────────────┘
```